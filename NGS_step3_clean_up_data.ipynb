{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "After inferring split sites on DNA and amino acid sequences, two csv files containing fastq IDs and split sites info are generated for each target protein.\n",
    "\n",
    "Sub-steps:  \n",
    "1. Concatentate the 2 CSV files (pair-end reads) together.\n",
    "2. Remove any records where the forward and reverse reads reported different split/insertion sites.  \n",
    "3. Deduplicate any records where the forward and reverse reads pointed to the same split/insertion site.\n",
    "4. Remove sites mapped beyond the permitted transposition windows. \n",
    "5. Work out the protein split sites that are missing from the library.\n",
    "6. Calculate percentage coverage on DNA and amino acid sequence levels.\n",
    "\n",
    "For sub-steps 4-5, information regarding DNA and amino acid sequence transposition windows are retrieved from a csv file named `target_protein_info.csv`, stored at the root directory.\n",
    "\n",
    "This notebook file uses the complete data of the mCherry library and therefore takes a lot of time to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to clean up data (Steps 3.2 - 3.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_aligned_data(target_protein, all_reads_df,\n",
    "                          five_prime_trans_border, three_prime_trans_border):\n",
    "    \n",
    "    # Step 2: Remove reads multiple insertions\n",
    "    # Find all reads that have duplicated ID\n",
    "    data_with_dup_IDs = all_reads_df[all_reads_df['fastq_id'].duplicated(keep=False)]\n",
    "    duplicated_ids = data_with_dup_IDs['fastq_id'].drop_duplicates().to_list()\n",
    "    \n",
    "    # Check if any data that contains duplicated IDs were actually mapped to different split sites\n",
    "    ids_with_multiple_insertion = list()\n",
    "    for query_id in duplicated_ids:\n",
    "        data_to_check = data_with_dup_IDs.query('fastq_id==@query_id')\n",
    "        if len(data_to_check) != 2:\n",
    "            print(\"More than two duplicated IDs\")\n",
    "            raise\n",
    "            \n",
    "        # to check if the split sites of the two records are identical,\n",
    "        # one only needs to check insertion orientation and dna split site\n",
    "        for col in ['insertion_orientation', 'dna_split_site_middle']:\n",
    "            if data_to_check.iloc[0][col] != data_to_check.iloc[1][col]:\n",
    "                ids_with_multiple_insertion.append(query_id)\n",
    "                break\n",
    "    \n",
    "    # Filter out records without double insertions\n",
    "    data_single_insertions = all_reads_df.query('~(fastq_id.isin(@ids_with_multiple_insertion))')\n",
    "    \n",
    "    # Step 3: Remove reads with same IDs and same insertions\n",
    "    # Remaining records still contain duplicated reads from paired ends that point to the same split site\n",
    "    data_deduplicated = data_single_insertions.drop_duplicates(subset=['fastq_id'], keep='first')\n",
    "    \n",
    "    # Step 4: Remove reads out of transposition windows\n",
    "    data_final = data_deduplicated.query('dna_split_site_middle>=@five_prime_trans_border and \\\n",
    "                dna_split_site_middle<=@three_prime_trans_border')\n",
    "    \n",
    "    return data_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read target protein information from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_target_protein_info = pd.read_csv(\"target_protein_info.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps 3.1 - 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_n_IDs = [\n",
    "    \"1_mCherry\" # Expand this list or use fnmatch for batch processing\n",
    "]\n",
    "\n",
    "for library_n_ID in library_n_IDs:\n",
    "    \n",
    "    target_protein = library_n_ID.split(\"_\")[1]\n",
    "    \n",
    "    # Get target protein info from imported csv file\n",
    "    target_protein_info_row = all_target_protein_info.query('target_protein==@target_protein')\n",
    "    end_dna = target_protein_info_row.iloc[0]['end_dna']\n",
    "    five_prime_trans_border = target_protein_info_row.iloc[0]['five_prime_trans_border']\n",
    "    three_prime_trans_border = target_protein_info_row.iloc[0]['three_prime_trans_border']\n",
    "    n_trans_border = target_protein_info_row.iloc[0]['n_trans_border']\n",
    "    c_trans_border = target_protein_info_row.iloc[0]['c_trans_border']\n",
    "    \n",
    "    # Read data\n",
    "    base_filename = \"IBM_NGS_aligned_reads_\" + library_n_ID\n",
    "    csv_filenames = [base_filename + \"_\" + str(i+1) + \".csv\" for i in range(2)]\n",
    "    aligned_reads_dfs = [pd.read_csv(os.path.join(\"results_per_fastq\", filename)) for filename in csv_filenames]\n",
    "    all_reads_df = pd.concat(aligned_reads_dfs, ignore_index=True, verify_integrity=True)\n",
    "    \n",
    "    # Clean up data\n",
    "    cleaned_up_data = clean_up_aligned_data(target_protein, all_reads_df,\n",
    "                          five_prime_trans_border, three_prime_trans_border)\n",
    "    output_filename = \"IBM_NGS_cleaned_up_reads_\" + library_n_ID +  \".csv\"\n",
    "    output_filepath = os.path.join(\"results_per_target_protein\", output_filename)\n",
    "    cleaned_up_data.to_csv(output_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps 3.5 - 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_split_site_name(list_of_sites):\n",
    "    new_list = [str(int(site-0.5)) + \"/\" + str(int(site+0.5)) for site in list_of_sites]\n",
    "    readable_list = str()\n",
    "    for site in new_list:\n",
    "        readable_list += site + \", \"\n",
    "    return readable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library_n_ID in library_n_IDs:\n",
    "    \n",
    "    # Reimport exported data\n",
    "    target_protein = library_n_ID.split(\"_\")[1]\n",
    "    output_filename = \"IBM_NGS_cleaned_up_reads_\" + library_n_ID +  \".csv\"\n",
    "    output_filepath = os.path.join(\"results_per_target_protein\", output_filename)\n",
    "    cleaned_up_data = pd.read_csv(output_filepath)\n",
    "    \n",
    "    # Get target protein info from imported csv file\n",
    "    target_protein_info_row = all_target_protein_info.query('target_protein==@target_protein')\n",
    "    end_dna = target_protein_info_row.iloc[0]['end_dna']\n",
    "    five_prime_trans_border = target_protein_info_row.iloc[0]['five_prime_trans_border']\n",
    "    three_prime_trans_border = target_protein_info_row.iloc[0]['three_prime_trans_border']\n",
    "    n_trans_border = target_protein_info_row.iloc[0]['n_trans_border']\n",
    "    c_trans_border = target_protein_info_row.iloc[0]['c_trans_border']\n",
    "    \n",
    "    # Step 5 - 6: Find missing insertions in protein sequence and calculate aa sequence coverage\n",
    "    productive_data = cleaned_up_data.query('productive_insertion==True')['aa_split_site_middle']\n",
    "    aa_sites_hitted = productive_data.drop_duplicates().sort_values().to_list()\n",
    "    theoretical_all_aa_sites = [i+0.5 for i in range(int(n_trans_border-0.5), int(c_trans_border+0.5))]\n",
    "    missing_aa_sites = [site for site in theoretical_all_aa_sites if site not in aa_sites_hitted]\n",
    "    aa_seq_coverage = round((len(theoretical_all_aa_sites)-len(missing_aa_sites))/len(theoretical_all_aa_sites), 3)\n",
    "    \n",
    "    # Step 6: Calculate DNA sequence coverage, for both FW and RV orientations intead of either one value for each strand\n",
    "    num_of_DNA_sites_hitted = len(cleaned_up_data.query('insertion_orientation==\"FW\"')['dna_split_site_middle'].drop_duplicates()) + \\\n",
    "                    len(cleaned_up_data.query('insertion_orientation==\"RV\"')['dna_split_site_middle'].drop_duplicates())\n",
    "    num_of_theoretical_all_dna_sites = 2 * int(three_prime_trans_border - five_prime_trans_border + 1)\n",
    "    dna_seq_coverage = round(num_of_DNA_sites_hitted / num_of_theoretical_all_dna_sites, 3)\n",
    "    \n",
    "    # Write result summary to text file\n",
    "    f = open(\"results_per_target_protein/NGS_all_aligned_results_summary.txt\", \"a+\")\n",
    "    f.write(\"-----------------------------------------\" + \"\\n\")\n",
    "    f.write(\"target protein = \" + target_protein + \"\\n\")\n",
    "    f.write(\"aligned read counts = \" +  str(len(cleaned_up_data)) + \"\\n\")\n",
    "    f.write(\"possible DNA insertion sites count =  \" + str(num_of_theoretical_all_dna_sites) + \"\\n\")\n",
    "    f.write(\"missing DNA insertion sites count =  \" + str(num_of_theoretical_all_dna_sites - num_of_DNA_sites_hitted) + \"\\n\")\n",
    "    f.write(\"DNA sequence insertion coverage =  \" + str(dna_seq_coverage) + \"\\n\")\n",
    "    f.write(\"possible amino acid split/insertion sites count =  \" + str(len(theoretical_all_aa_sites)) + \"\\n\")\n",
    "    f.write(\"missing aa split/insertion sites count= \" + str(len(missing_aa_sites)) + \"\\n\")\n",
    "    f.write(\"missing aa split/insertion sites = \" + str(convert_split_site_name(missing_aa_sites)) + \"\\n\")\n",
    "    f.write(\"aa sequence coverage =  \" + str(aa_seq_coverage) + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "target protein = mCherry\n",
      "aligned read counts = 1012384\n",
      "possible DNA insertion sites count =  1138\n",
      "missing DNA insertion sites count =  125\n",
      "DNA sequence insertion coverage =  0.89\n",
      "possible amino acid split/insertion sites count =  189\n",
      "missing aa split/insertion sites count= 11\n",
      "missing aa split/insertion sites = 30/31, 36/37, 40/41, 51/52, 78/79, 127/128, 130/131, 166/167, 170/171, 193/194, 216/217, \n",
      "aa sequence coverage =  0.942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"results_per_target_protein/NGS_all_aligned_results_summary.txt\", 'r')\n",
    "print(f.read())\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
